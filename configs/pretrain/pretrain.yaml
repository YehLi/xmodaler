####################################### DATASETS #######################################
DATASETS:
  TRAIN: 'ConceptualCaptionsDataset'
  VAL: ''
  TEST: ''

###################################### DATALOADER ######################################
DATALOADER:
  TRAIN_BATCH_SIZE: 64
  TEST_BATCH_SIZE: 32
  NUM_WORKERS: 4
  FEATS_FOLDER: '../open_source_dataset/ConceptualCaptions/features/up_down'
  ANNO_FOLDER: '../open_source_dataset/ConceptualCaptions'
  SEQ_PER_SAMPLE:  1
  MAX_FEAT_NUM: 51

######################################### Engine #########################################
ENGINE:
  NAME: 'TDENPretrainer'

######################################### MODEL #########################################
MODEL:
  VOCAB_SIZE: 30522 # include <BOS>/<EOS>
  META_ARCHITECTURE: 'TDENPretrain'
  ENCODER: 'TwoStreamBertEncoder'
  DECODER: 'DecoupleBertDecoder'
  PREDICTOR: 'BertPredictionHead'
  V_PREDICTOR: 'BertVisualPredictionHead'
  MAX_SEQ_LEN: 36
  WEIGHTS: '../pretrain/BERT/bert-base-uncased.pth'

######################################### BERT #########################################
  BERT:
    HIDDEN_SIZE: 768
    HIDDEN_DROPOUT_PROB: 0.1
    HIDDEN_ACT: "gelu"
    NUM_ATTENTION_HEADS: 12
    INTERMEDIATE_SIZE: 3072
    INTERMEDIATE_DROP: 0.1
    FFN_DROPOUT_PROB: 0.1
    ATTENTION_PROBS_DROPOUT_PROB: 0.1
    V_TARGET_SIZE: 1601
    NUM_HIDDEN_LAYERS: 12
    V_NUM_HIDDEN_LAYERS: 6
    NUM_UNDERSTANDING_LAYERS: 6
    NUM_GENERATION_LAYERS: 6
    LAYER_DROP: 0.0
    V_LAYER_DROP: 0.0
    U_LAYER_DROP: 0.0
    G_LAYER_DROP: 0.0

#################################### Token embedding ####################################
  TOKEN_EMBED:
    NAME: 'TokenBaseEmbedding'
    DIM: 768
    ACTIVATION: 'none'
    USE_NORM: True
    DROPOUT: 0.1
    POSITION: 'NNEmbeddingEncoding'
    POSITION_MAX_LEN: 512
    TYPE_VOCAB_SIZE: 3

#################################### Visual embedding ####################################
  VISUAL_EMBED:
    NAME: 'VisualBaseEmbedding'
    IN_DIM: 2048
    OUT_DIM: 768
    LOCATION_SIZE: 5
    ACTIVATION: 'none'
    USE_NORM: True
    DROPOUT: 0.1
######################################### MM_PREDICTOR #########################################
  MM_PREDICTOR:
    POOLER_INPUT_SIZE: 768
    POOLER_OUTPUT_SIZE: 1024
    POOLER_BN: True
    POOLER_DROPOUT: 0.1
    
####################################### Optimizer #######################################
SOLVER:
  NAME: 'AdamW'
  EPOCH: 10
  CHECKPOINT_PERIOD: 1
  EVAL_PERIOD: 1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  WEIGHT_DECAY: 0.01
  WEIGHT_DECAY_NORM: 0.0
  WEIGHT_DECAY_BIAS: 0.0
  MOMENTUM: 0.9
  DAMPENING: 0.0
  NESTEROV: 0.0
  BETAS: [0.9, 0.999]
  EPS: 1e-8
  GRAD_CLIP: -1.0
  GRAD_CLIP_TYPE: 'value'
  NORM_TYPE: 2.0
  
####################################### lr scheduler ####################################### 
LR_SCHEDULER:
  NAME: 'WarmupLinear'
  WARMUP: 1
  MIN_LR: 0.0

####################################### decode strategy ####################################### 
DECODE_STRATEGY:
  NAME: 'BeamSearcher'
  BEAM_SIZE: 3

####################################### losses ####################################### 
LOSSES:
  NAMES: ['PretrainLosses']

INFERENCE:
  VOCAB: ''
