

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>xmodaler.tokenization.tokenization_bert &mdash; Xmodaler 1.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> Xmodaler
          

          
          </a>

          
            
            
              <div class="version">
                v 1.0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/index.html">Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../modules/index.html">API Documentation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Xmodaler</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>xmodaler.tokenization.tokenization_bert</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for xmodaler.tokenization.tokenization_bert</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;Tokenization classes.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">unicode_literals</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="nb">open</span>

<span class="kn">from</span> <span class="nn">pytorch_transformers.tokenization_utils</span> <span class="kn">import</span> <span class="n">PreTrainedTokenizer</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">VOCAB_FILES_NAMES</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;vocab_file&#39;</span><span class="p">:</span> <span class="s1">&#39;vocab.txt&#39;</span><span class="p">}</span>

<span class="n">PRETRAINED_VOCAB_FILES_MAP</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;vocab_file&#39;</span><span class="p">:</span>
    <span class="p">{</span>
        <span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">:</span> <span class="s2">&quot;../bert-base-uncased-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-large-uncased&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-base-cased&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-large-cased&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-base-multilingual-uncased&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-base-multilingual-cased&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-base-chinese&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-base-german-cased&#39;</span><span class="p">:</span> <span class="s2">&quot;https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-large-uncased-whole-word-masking&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-large-cased-whole-word-masking&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-large-uncased-whole-word-masking-finetuned-squad&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-large-cased-whole-word-masking-finetuned-squad&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-vocab.txt&quot;</span><span class="p">,</span>
        <span class="s1">&#39;bert-base-cased-finetuned-mrpc&#39;</span><span class="p">:</span> <span class="s2">&quot;https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-vocab.txt&quot;</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-large-uncased&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-base-cased&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-large-cased&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-base-multilingual-uncased&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-base-multilingual-cased&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-base-chinese&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-base-german-cased&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-large-uncased-whole-word-masking&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-large-cased-whole-word-masking&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-large-uncased-whole-word-masking-finetuned-squad&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-large-cased-whole-word-masking-finetuned-squad&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
    <span class="s1">&#39;bert-base-cased-finetuned-mrpc&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">PRETRAINED_INIT_CONFIGURATION</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
    <span class="s1">&#39;bert-large-uncased&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
    <span class="s1">&#39;bert-base-cased&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="s1">&#39;bert-large-cased&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="s1">&#39;bert-base-multilingual-uncased&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
    <span class="s1">&#39;bert-base-multilingual-cased&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="s1">&#39;bert-base-chinese&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="s1">&#39;bert-base-german-cased&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="s1">&#39;bert-large-uncased-whole-word-masking&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
    <span class="s1">&#39;bert-large-cased-whole-word-masking&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="s1">&#39;bert-large-uncased-whole-word-masking-finetuned-squad&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
    <span class="s1">&#39;bert-large-cased-whole-word-masking-finetuned-squad&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="s1">&#39;bert-base-cased-finetuned-mrpc&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;do_lower_case&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
<span class="p">}</span>


<span class="k">def</span> <span class="nf">load_vocab</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Loads a vocabulary file into a dictionary.&quot;&quot;&quot;</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">()</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
    <span class="k">return</span> <span class="n">vocab</span>


<span class="k">def</span> <span class="nf">whitespace_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Runs basic whitespace cleaning and splitting on a piece of text.&quot;&quot;&quot;</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">tokens</span>


<div class="viewcode-block" id="BertTokenizer"><a class="viewcode-back" href="../../../modules/tokenization.html#xmodaler.tokenization.BertTokenizer">[docs]</a><span class="k">class</span> <span class="nc">BertTokenizer</span><span class="p">(</span><span class="n">PreTrainedTokenizer</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a BertTokenizer.</span>
<span class="sd">    :class:`~pytorch_transformers.BertTokenizer` runs end-to-end tokenization: punctuation splitting + wordpiece</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_file: Path to a one-wordpiece-per-line vocabulary file</span>
<span class="sd">        do_lower_case: Whether to lower case the input. Only has an effect when do_wordpiece_only=False</span>
<span class="sd">        do_basic_tokenize: Whether to do basic tokenization before wordpiece.</span>
<span class="sd">        max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the</span>
<span class="sd">            minimum of this value (if specified) and the underlying BERT model&#39;s sequence length.</span>
<span class="sd">        never_split: List of tokens which will never be split during tokenization. Only has an effect when</span>
<span class="sd">            do_wordpiece_only=False</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">vocab_files_names</span> <span class="o">=</span> <span class="n">VOCAB_FILES_NAMES</span>
    <span class="n">pretrained_vocab_files_map</span> <span class="o">=</span> <span class="n">PRETRAINED_VOCAB_FILES_MAP</span>
    <span class="n">pretrained_init_configuration</span> <span class="o">=</span> <span class="n">PRETRAINED_INIT_CONFIGURATION</span>
    <span class="n">max_model_input_sizes</span> <span class="o">=</span> <span class="n">PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_file</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">do_basic_tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">never_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="n">sep_token</span><span class="o">=</span><span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="n">cls_token</span><span class="o">=</span><span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span>
                 <span class="n">mask_token</span><span class="o">=</span><span class="s2">&quot;[MASK]&quot;</span><span class="p">,</span> <span class="n">tokenize_chinese_chars</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Constructs a BertTokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            **vocab_file**: Path to a one-wordpiece-per-line vocabulary file</span>
<span class="sd">            **do_lower_case**: (`optional`) boolean (default True)</span>
<span class="sd">                Whether to lower case the input</span>
<span class="sd">                Only has an effect when do_basic_tokenize=True</span>
<span class="sd">            **do_basic_tokenize**: (`optional`) boolean (default True)</span>
<span class="sd">                Whether to do basic tokenization before wordpiece.</span>
<span class="sd">            **never_split**: (`optional`) list of string</span>
<span class="sd">                List of tokens which will never be split during tokenization.</span>
<span class="sd">                Only has an effect when do_basic_tokenize=True</span>
<span class="sd">            **tokenize_chinese_chars**: (`optional`) boolean (default True)</span>
<span class="sd">                Whether to tokenize Chinese characters.</span>
<span class="sd">                This should likely be deactivated for Japanese:</span>
<span class="sd">                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BertTokenizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="n">unk_token</span><span class="p">,</span> <span class="n">sep_token</span><span class="o">=</span><span class="n">sep_token</span><span class="p">,</span>
                                            <span class="n">pad_token</span><span class="o">=</span><span class="n">pad_token</span><span class="p">,</span> <span class="n">cls_token</span><span class="o">=</span><span class="n">cls_token</span><span class="p">,</span>
                                            <span class="n">mask_token</span><span class="o">=</span><span class="n">mask_token</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_len_single_sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span> <span class="o">-</span> <span class="mi">2</span>  <span class="c1"># take into account special tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_len_sentences_pair</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_len</span> <span class="o">-</span> <span class="mi">3</span>  <span class="c1"># take into account special tokens</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t find a vocabulary file at path &#39;</span><span class="si">{}</span><span class="s2">&#39;. To load the vocabulary from a Google pretrained &quot;</span>
                <span class="s2">&quot;model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">load_vocab</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ids_to_tokens</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">(</span>
            <span class="p">[(</span><span class="n">ids</span><span class="p">,</span> <span class="n">tok</span><span class="p">)</span> <span class="k">for</span> <span class="n">tok</span><span class="p">,</span> <span class="n">ids</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_basic_tokenize</span> <span class="o">=</span> <span class="n">do_basic_tokenize</span>
        <span class="k">if</span> <span class="n">do_basic_tokenize</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">basic_tokenizer</span> <span class="o">=</span> <span class="n">BasicTokenizer</span><span class="p">(</span><span class="n">do_lower_case</span><span class="o">=</span><span class="n">do_lower_case</span><span class="p">,</span>
                                                  <span class="n">never_split</span><span class="o">=</span><span class="n">never_split</span><span class="p">,</span>
                                                  <span class="n">tokenize_chinese_chars</span><span class="o">=</span><span class="n">tokenize_chinese_chars</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wordpiece_tokenizer</span> <span class="o">=</span> <span class="n">WordpieceTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">,</span> <span class="n">unk_token</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vocab_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="n">split_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_basic_tokenize</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">basic_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">never_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">sub_token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">wordpiece_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
                    <span class="n">split_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sub_token</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">split_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wordpiece_tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">split_tokens</span>

    <span class="k">def</span> <span class="nf">_convert_token_to_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Converts a token (str/unicode) in an id using the vocab. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_convert_id_to_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Converts an index (integer) in a token (string/unicode) using the vocab.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ids_to_tokens</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)</span>

<div class="viewcode-block" id="BertTokenizer.convert_tokens_to_string"><a class="viewcode-back" href="../../../modules/tokenization.html#xmodaler.tokenization.BertTokenizer.convert_tokens_to_string">[docs]</a>    <span class="k">def</span> <span class="nf">convert_tokens_to_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Converts a sequence of tokens (string) in a single string. &quot;&quot;&quot;</span>
        <span class="n">out_string</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; ##&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">out_string</span></div>

<div class="viewcode-block" id="BertTokenizer.add_special_tokens_single_sentence"><a class="viewcode-back" href="../../../modules/tokenization.html#xmodaler.tokenization.BertTokenizer.add_special_tokens_single_sentence">[docs]</a>    <span class="k">def</span> <span class="nf">add_special_tokens_single_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds special tokens to the a sequence for sequence classification tasks.</span>
<span class="sd">        A BERT sequence has the following format: [CLS] X [SEP]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token_id</span><span class="p">]</span> <span class="o">+</span> <span class="n">token_ids</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sep_token_id</span><span class="p">]</span></div>

<div class="viewcode-block" id="BertTokenizer.add_special_tokens_sentences_pair"><a class="viewcode-back" href="../../../modules/tokenization.html#xmodaler.tokenization.BertTokenizer.add_special_tokens_sentences_pair">[docs]</a>    <span class="k">def</span> <span class="nf">add_special_tokens_sentences_pair</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">,</span> <span class="n">token_ids_1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds special tokens to a sequence pair for sequence classification tasks.</span>
<span class="sd">        A BERT sequence pair has the following format: [CLS] A [SEP] B [SEP]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sep</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">sep_token_id</span><span class="p">]</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token_id</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">cls</span> <span class="o">+</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">sep</span> <span class="o">+</span> <span class="n">token_ids_1</span> <span class="o">+</span> <span class="n">sep</span></div>

<div class="viewcode-block" id="BertTokenizer.save_vocabulary"><a class="viewcode-back" href="../../../modules/tokenization.html#xmodaler.tokenization.BertTokenizer.save_vocabulary">[docs]</a>    <span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_path</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Save the tokenizer vocabulary to a directory or file.&quot;&quot;&quot;</span>
        <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">):</span>
            <span class="n">vocab_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">,</span> <span class="n">VOCAB_FILES_NAMES</span><span class="p">[</span><span class="s1">&#39;vocab_file&#39;</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vocab_file</span> <span class="o">=</span> <span class="n">vocab_path</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">token_index</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">kv</span><span class="p">:</span> <span class="n">kv</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="k">if</span> <span class="n">index</span> <span class="o">!=</span> <span class="n">token_index</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Saving vocabulary to </span><span class="si">{}</span><span class="s2">: vocabulary indices are not consecutive.&quot;</span>
                                   <span class="s2">&quot; Please check that the vocabulary is not corrupted!&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">vocab_file</span><span class="p">))</span>
                    <span class="n">index</span> <span class="o">=</span> <span class="n">token_index</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">token</span> <span class="o">+</span> <span class="sa">u</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
                <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">vocab_file</span><span class="p">,)</span></div></div>


<span class="k">class</span> <span class="nc">BasicTokenizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Runs basic tokenization (punctuation splitting, lower casing, etc.).&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">never_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokenize_chinese_chars</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Constructs a BasicTokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            **do_lower_case**: Whether to lower case the input.</span>
<span class="sd">            **never_split**: (`optional`) list of str</span>
<span class="sd">                Kept for backward compatibility purposes.</span>
<span class="sd">                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)</span>
<span class="sd">                List of token not to split.</span>
<span class="sd">            **tokenize_chinese_chars**: (`optional`) boolean (default True)</span>
<span class="sd">                Whether to tokenize Chinese characters.</span>
<span class="sd">                This should likely be deactivated for Japanese:</span>
<span class="sd">                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">never_split</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">never_split</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_lower_case</span> <span class="o">=</span> <span class="n">do_lower_case</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">never_split</span> <span class="o">=</span> <span class="n">never_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenize_chinese_chars</span> <span class="o">=</span> <span class="n">tokenize_chinese_chars</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">never_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Basic Tokenization of a piece of text.</span>
<span class="sd">            Split on &quot;white spaces&quot; only, for sub-word tokenization, see WordPieceTokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            **never_split**: (`optional`) list of str</span>
<span class="sd">                Kept for backward compatibility purposes.</span>
<span class="sd">                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)</span>
<span class="sd">                List of token not to split.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">never_split</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">never_split</span> <span class="o">+</span> <span class="p">(</span><span class="n">never_split</span> <span class="k">if</span> <span class="n">never_split</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[])</span>
        <span class="n">text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="c1"># This was added on November 1st, 2018 for the multilingual and Chinese</span>
        <span class="c1"># models. This is also applied to the English models now, but it doesn&#39;t</span>
        <span class="c1"># matter since the English models were not trained on any Chinese data</span>
        <span class="c1"># and generally don&#39;t have any Chinese data in them (there are Chinese</span>
        <span class="c1"># characters in the vocabulary because Wikipedia does have some Chinese</span>
        <span class="c1"># words in the English Wikipedia.).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenize_chinese_chars</span><span class="p">:</span>
            <span class="n">text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenize_chinese_chars</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">orig_tokens</span> <span class="o">=</span> <span class="n">whitespace_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">split_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">orig_tokens</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_lower_case</span> <span class="ow">and</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">never_split</span><span class="p">:</span>
                <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
                <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_strip_accents</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="n">split_tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_run_split_on_punc</span><span class="p">(</span><span class="n">token</span><span class="p">))</span>

        <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">whitespace_tokenize</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">split_tokens</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">output_tokens</span>

    <span class="k">def</span> <span class="nf">_run_strip_accents</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Strips accents from a piece of text.&quot;&quot;&quot;</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s2">&quot;NFD&quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
            <span class="n">cat</span> <span class="o">=</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">cat</span> <span class="o">==</span> <span class="s2">&quot;Mn&quot;</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_run_split_on_punc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">never_split</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Splits punctuation on a piece of text.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">never_split</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">never_split</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">text</span><span class="p">]</span>
        <span class="n">chars</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">start_new_word</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">):</span>
            <span class="n">char</span> <span class="o">=</span> <span class="n">chars</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">_is_punctuation</span><span class="p">(</span><span class="n">char</span><span class="p">):</span>
                <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">char</span><span class="p">])</span>
                <span class="n">start_new_word</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">start_new_word</span><span class="p">:</span>
                    <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
                <span class="n">start_new_word</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_tokenize_chinese_chars</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds whitespace around any CJK character.&quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
            <span class="n">cp</span> <span class="o">=</span> <span class="nb">ord</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_chinese_char</span><span class="p">(</span><span class="n">cp</span><span class="p">):</span>
                <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
                <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
                <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_is_chinese_char</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cp</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Checks whether CP is the codepoint of a CJK character.&quot;&quot;&quot;</span>
        <span class="c1"># This defines a &quot;chinese character&quot; as anything in the CJK Unicode block:</span>
        <span class="c1">#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)</span>
        <span class="c1">#</span>
        <span class="c1"># Note that the CJK Unicode block is NOT all Japanese and Korean characters,</span>
        <span class="c1"># despite its name. The modern Korean Hangul alphabet is a different block,</span>
        <span class="c1"># as is Japanese Hiragana and Katakana. Those alphabets are used to write</span>
        <span class="c1"># space-separated words, so they are not treated specially and handled</span>
        <span class="c1"># like the all of the other languages.</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mh">0x4E00</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mh">0x9FFF</span><span class="p">)</span> <span class="ow">or</span>  <span class="c1">#</span>
                <span class="p">(</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mh">0x3400</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mh">0x4DBF</span><span class="p">)</span> <span class="ow">or</span>  <span class="c1">#</span>
                <span class="p">(</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mh">0x20000</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mh">0x2A6DF</span><span class="p">)</span> <span class="ow">or</span>  <span class="c1">#</span>
                <span class="p">(</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mh">0x2A700</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mh">0x2B73F</span><span class="p">)</span> <span class="ow">or</span>  <span class="c1">#</span>
                <span class="p">(</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mh">0x2B740</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mh">0x2B81F</span><span class="p">)</span> <span class="ow">or</span>  <span class="c1">#</span>
                <span class="p">(</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mh">0x2B820</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mh">0x2CEAF</span><span class="p">)</span> <span class="ow">or</span>
                <span class="p">(</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mh">0xF900</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mh">0xFAFF</span><span class="p">)</span> <span class="ow">or</span>  <span class="c1">#</span>
                <span class="p">(</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mh">0x2F800</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mh">0x2FA1F</span><span class="p">)):</span>  <span class="c1">#</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_clean_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs invalid character removal and whitespace cleanup on text.&quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
            <span class="n">cp</span> <span class="o">=</span> <span class="nb">ord</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">cp</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">cp</span> <span class="o">==</span> <span class="mh">0xfffd</span> <span class="ow">or</span> <span class="n">_is_control</span><span class="p">(</span><span class="n">char</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="n">_is_whitespace</span><span class="p">(</span><span class="n">char</span><span class="p">):</span>
                <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">WordpieceTokenizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Runs WordPiece tokenization.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">unk_token</span><span class="p">,</span> <span class="n">max_input_chars_per_word</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span> <span class="o">=</span> <span class="n">unk_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_input_chars_per_word</span> <span class="o">=</span> <span class="n">max_input_chars_per_word</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Tokenizes a piece of text into its word pieces.</span>

<span class="sd">        This uses a greedy longest-match-first algorithm to perform tokenization</span>
<span class="sd">        using the given vocabulary.</span>

<span class="sd">        For example:</span>
<span class="sd">          input = &quot;unaffable&quot;</span>
<span class="sd">          output = [&quot;un&quot;, &quot;##aff&quot;, &quot;##able&quot;]</span>

<span class="sd">        Args:</span>
<span class="sd">          text: A single token or whitespace separated tokens. This should have</span>
<span class="sd">            already been passed through `BasicTokenizer`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A list of wordpiece tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">output_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">whitespace_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
            <span class="n">chars</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_input_chars_per_word</span><span class="p">:</span>
                <span class="n">output_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="n">is_bad</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">sub_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">):</span>
                <span class="n">end</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
                <span class="n">cur_substr</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">:</span>
                    <span class="n">substr</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chars</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>
                    <span class="k">if</span> <span class="n">start</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">substr</span> <span class="o">=</span> <span class="s2">&quot;##&quot;</span> <span class="o">+</span> <span class="n">substr</span>
                    <span class="k">if</span> <span class="n">substr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
                        <span class="n">cur_substr</span> <span class="o">=</span> <span class="n">substr</span>
                        <span class="k">break</span>
                    <span class="n">end</span> <span class="o">-=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="n">cur_substr</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">is_bad</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">break</span>
                <span class="n">sub_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_substr</span><span class="p">)</span>
                <span class="n">start</span> <span class="o">=</span> <span class="n">end</span>

            <span class="k">if</span> <span class="n">is_bad</span><span class="p">:</span>
                <span class="n">output_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sub_tokens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output_tokens</span>


<span class="k">def</span> <span class="nf">_is_whitespace</span><span class="p">(</span><span class="n">char</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Checks whether `chars` is a whitespace character.&quot;&quot;&quot;</span>
    <span class="c1"># \t, \n, and \r are technically contorl characters but we treat them</span>
    <span class="c1"># as whitespace since they are generally considered as such.</span>
    <span class="k">if</span> <span class="n">char</span> <span class="o">==</span> <span class="s2">&quot; &quot;</span> <span class="ow">or</span> <span class="n">char</span> <span class="o">==</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span> <span class="ow">or</span> <span class="n">char</span> <span class="o">==</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="ow">or</span> <span class="n">char</span> <span class="o">==</span> <span class="s2">&quot;</span><span class="se">\r</span><span class="s2">&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="n">cat</span> <span class="o">=</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cat</span> <span class="o">==</span> <span class="s2">&quot;Zs&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_is_control</span><span class="p">(</span><span class="n">char</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Checks whether `chars` is a control character.&quot;&quot;&quot;</span>
    <span class="c1"># These are technically control characters but we count them as whitespace</span>
    <span class="c1"># characters.</span>
    <span class="k">if</span> <span class="n">char</span> <span class="o">==</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span> <span class="ow">or</span> <span class="n">char</span> <span class="o">==</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="ow">or</span> <span class="n">char</span> <span class="o">==</span> <span class="s2">&quot;</span><span class="se">\r</span><span class="s2">&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="n">cat</span> <span class="o">=</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cat</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_is_punctuation</span><span class="p">(</span><span class="n">char</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Checks whether `chars` is a punctuation character.&quot;&quot;&quot;</span>
    <span class="n">cp</span> <span class="o">=</span> <span class="nb">ord</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
    <span class="c1"># We treat all non-letter/number ASCII as punctuation.</span>
    <span class="c1"># Characters such as &quot;^&quot;, &quot;$&quot;, and &quot;`&quot; are not in the Unicode</span>
    <span class="c1"># Punctuation class but we treat them as punctuation anyways, for</span>
    <span class="c1"># consistency.</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mi">33</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mi">47</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mi">58</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mi">64</span><span class="p">)</span> <span class="ow">or</span>
            <span class="p">(</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mi">91</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mi">96</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">cp</span> <span class="o">&gt;=</span> <span class="mi">123</span> <span class="ow">and</span> <span class="n">cp</span> <span class="o">&lt;=</span> <span class="mi">126</span><span class="p">)):</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="n">cat</span> <span class="o">=</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cat</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;P&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>

</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Yehao Li.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>